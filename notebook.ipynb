{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "import argparse\n",
    "\n",
    "MEX_NEW_TOKENS = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at C:\\Users\\Marwa/.cache\\huggingface\\hub\\models--TheBloke--falcon-7b-instruct-GPTQ\\snapshots\\78e749e37afef2600decdc389a5c65c82c7589f4\\gptq_model-4bit-64g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "can't get model's sequence length from model config, will set to 4096.\n",
      "RWGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "RWGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "The model 'RWGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'RWForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/falcon-7b-instruct-GPTQ\"\n",
    "\n",
    "model_basename = \"gptq_model-4bit-64g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer_falcon = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model_falcon = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "pipe_falcon = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_falcon,\n",
    "    tokenizer=tokenizer_falcon,\n",
    "    max_new_tokens=16,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    pad_token_id=tokenizer_falcon.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "\n",
    "pipe_gpt = pipeline('text-generation', model='openai-gpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "custom_metric = evaluate.load(\"./custom_metric.py\")\n",
    "\n",
    "input_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"To be or not to be, that is the\",\n",
    "    \"The quick brown fox jumps over the\",\n",
    "]\n",
    "\n",
    "reference_texts = [\n",
    "    [\"Once upon a time, there was a beautiful princess.\"],\n",
    "    [\"To be or not to be, that is the question.\"],\n",
    "    [\"The quick brown fox jumps over the lazy dog.\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.4 s\n",
      "Wall time: 34.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Once upon a time, there was a little girl named Alice. She was a curious little girl,',\n",
       " 'To be or not to be, that is the question:',\n",
       " 'The quick brown fox jumps over the lazy dog.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions_faclon = [pipe_falcon(prompt)[0][\"generated_text\"].split('\\n')[0] for prompt in input_prompts]\n",
    "predictions_faclon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.78 s\n",
      "Wall time: 1.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Once upon a time, and i 'd heard all that, but i knew i\",\n",
       " 'To be or not to be, that is the case, \" he answered coldly',\n",
       " 'The quick brown fox jumps over the side on long, black legs, and leaps']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions_gpt = [pipe_gpt(prompt, max_length=MEX_NEW_TOKENS, num_return_sequences=1)[0][\"generated_text\"].split('\\n')[0] for prompt in input_prompts]\n",
    "predictions_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falcon: {'sacrebleu_score': 65.52854486644404}\n",
      "gpt: {'sacrebleu_score': 39.5929619041636}\n"
     ]
    }
   ],
   "source": [
    "score_falcon = custom_metric.compute(predictions=predictions_faclon, references=reference_texts)\n",
    "print(\"falcon:\", score_falcon)\n",
    "score_gpt = custom_metric.compute(predictions=predictions_gpt, references=reference_texts)\n",
    "print(\"gpt:\", score_gpt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
