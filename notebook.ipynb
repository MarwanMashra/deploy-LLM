{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "import argparse\n",
    "\n",
    "MEX_NEW_TOKENS = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/falcon-7b-instruct-GPTQ\"\n",
    "\n",
    "model_basename = \"gptq_model-4bit-64g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer_falcon = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model_falcon = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "pipe_falcon = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_falcon,\n",
    "    tokenizer=tokenizer_falcon,\n",
    "    max_new_tokens=16,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    pad_token_id=tokenizer_falcon.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "\n",
    "pipe_gpt = pipeline('text-generation', model='openai-gpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "custom_metric = evaluate.load(\"./custom_metric.py\")\n",
    "\n",
    "input_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"To be or not to be, that is the\",\n",
    "    \"The quick brown fox jumps over the\",\n",
    "]\n",
    "\n",
    "reference_texts = [\n",
    "    [\"Once upon a time, there was a beautiful princess.\"],\n",
    "    [\"To be or not to be, that is the question.\"],\n",
    "    [\"The quick brown fox jumps over the lazy dog.\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.4 s\n",
      "Wall time: 34.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Once upon a time, there was a little girl named Alice. She was a curious little girl,',\n",
       " 'To be or not to be, that is the question:',\n",
       " 'The quick brown fox jumps over the lazy dog.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions_faclon = [pipe_falcon(prompt)[0][\"generated_text\"].split('\\n')[0] for prompt in input_prompts]\n",
    "predictions_faclon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.78 s\n",
      "Wall time: 1.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Once upon a time, and i 'd heard all that, but i knew i\",\n",
       " 'To be or not to be, that is the case, \" he answered coldly',\n",
       " 'The quick brown fox jumps over the side on long, black legs, and leaps']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions_gpt = [pipe_gpt(prompt, max_length=MEX_NEW_TOKENS, num_return_sequences=1)[0][\"generated_text\"].split('\\n')[0] for prompt in input_prompts]\n",
    "predictions_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falcon: {'sacrebleu_score': 65.52854486644404}\n",
      "gpt: {'sacrebleu_score': 39.5929619041636}\n"
     ]
    }
   ],
   "source": [
    "score_falcon = custom_metric.compute(predictions=predictions_faclon, references=reference_texts)\n",
    "print(\"falcon:\", score_falcon)\n",
    "score_gpt = custom_metric.compute(predictions=predictions_gpt, references=reference_texts)\n",
    "print(\"gpt:\", score_gpt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
